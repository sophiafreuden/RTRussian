---
title: "RTRussian"
author: "Sophia Freuden"
date: "1/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(tm)
library(wordcloud)
library(RColorBrewer)
library(RCurl)
library(corpus)
library(textclean)
library(lubridate)
library(gt)
library(tidyverse)
```

```{r}
col_types = cols(
  date = col_character(),
  title = col_character(),
  content = col_character(),
  URL = col_character()
)
```

```{r}
# Select which file you want to read and leave the rest commented out.

data1 <- read_delim("klinton-grazhdanskoe-obschestvo/klinton-grazhdanskoe-obschestvo.txt", delim = "*", col_types = col_types)
data2 <- read_delim("klinton-maidan/klinton-maidan.txt", delim = "*", col_types = col_types)
data3 <- read_delim("klinton-oranzhevaya-revolyutsiya/klinton-oranzhevaya-revolyutsiya.txt", delim = "*", col_types = col_types)
data4 <- read_delim("klinton-protesty/klinton-protesty.txt", delim = "*", col_types = col_types)
data5 <- read_delim("klinton-nato/klinton-nato.txt", delim = "*", col_types = col_types)

# data <- read_delim("makfol/makfol.txt", delim = "*", col_types = col_types)
# data <- read_delim("soros/soros.txt", delim = "*", col_types = col_types)

# for soros.txt:
# data <- data[-c(69, 114, 126, 168, 199),] # Adjust numbers to be rows where content = NA
# for makfol.txt:
# data <- data[-c(17, 21),]
```

```{r}
# clinton

data12 <- bind_rows(data1, data2)
data123 <- bind_rows(data12, data3)
data1234 <- bind_rows(data123, data4)
all_clinton <- bind_rows(data1234, data5)

data <- unique(all_clinton)
```

```{r}
data <- data %>% 
  filter(date != "Skipped")

data <- data %>% # This usually just strips all the commas out of the date column.
  mutate(date = dmy(date)) # Be sure to double check, however.
```

```{r}
# view(data)
```

```{r}
corpus <- SimpleCorpus(VectorSource(data$content))
```

```{r}
corpus <- tm_map(corpus, stripWhitespace)

corpus <- tm_map(corpus, content_transformer(tolower))

corpus <- tm_map(corpus, removeNumbers)

corpus <- tm_map(corpus, removePunctuation)

corpus <- tm_map(corpus, removeWords, stopwords("russian"))
```

```{r}
nonstem.corpus <- corpus

# view(nonstem.corpus)
```

```{r}
nDTM <- DocumentTermMatrix(nonstem.corpus)
```

```{r}
sums <- as.data.frame(colSums(as.matrix(nDTM)))
sums <- rownames_to_column(sums) 
colnames(sums) <- c("word", "count")
sums <- arrange(sums, desc(count))
head <- sums[1:75,]

# view(head)

sums2 <- as.data.frame(as.matrix(nDTM))
```

```{r}
sums2$ArtDate <- data$date

# To get rid of the redundancies, I add the URL here and then use the unique function to parse out redundant stories. While the URL itself isn't necessary for anything, it does serve as a helpful unique ID code that I can use to filter out redundant stories.

sums2$URL <- data$URL
sums2 <- unique(sums2)

# view(sums2)
```

```{r}
names <- colnames(sums2)

names2 <- text_tokens(names, stemmer = "ru")

names3 <- sapply(names2, `[[`, 1) # Sometimes this throws an error. Because I haven't found a use for sums2d, I don't think it really matters. It still seems to work despite the error.

names4 <- unlist(names3, use.names=FALSE)

sums2d <- sums2

colnames(sums2d) <- names4 # This creates a stemmed version of the file. Unfortunately,
# there will be columns that have the same name because of how Russian works. Adding these
# columns together in R seems like a hassle. If there are issues plotting with the stemmed
# dataset (sums2d), it might be better to use the non-stemmed one (sums2).

# view(sums2)
# view(sums2d) Trying to use this one, it doesn't work as there are columns with the same names. In that case, I might just have to keep using the non-stemmed version and acknowledge
# its incomplete accuracy.

# You will often see English/non-Russian words copied into the article content. I've decided
# to leave those as is.
```

```{r}
choice2 <- sums2 %>% select(c(ArtDate, запада, URL))
choice2 <- unique(choice2)
```

```{r}
sums2 %>%
  group_by(ArtDate) %>% 
  summarise(Frequency = sum(запада)) %>%
  ggplot(aes(x = ArtDate, y = Frequency)) +
  geom_point() +
  # geom_smooth(method = 'loess') +
  labs(
    title = "Term Frequency Per Article Over Time",
    subtitle = "Term: 'запада' (west), RT Search Term: N/A, 'Клинтон' (Clinton) Terms",
    x = "Date",
    y = "Frequency"
    # Comment/uncomment caption as neeed
    # caption = "Declined terms combined."
  ) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggsave("all-klinton/west2.png", width = 10)
```

```{r}
# Change min.freq below as needed per resultsno pulled from Python scrape.
wordcloud(words = head$word, freq = head$count, min.freq = 300,
  max.words=100, random.order=FALSE, rot.per=0.35, 
  colors=brewer.pal(8, "Dark2"))
```

```{r}
col_types2 <- cols(
  word = col_character(),
  score = col_number()
)

rudict <- read_csv("russiandict.csv", col_types = col_types2)

# view(rudict$score)
```

```{r}
ru_sentiment <- sums %>% 
  inner_join(rudict, by = "word")

# view(ru_sentiment$score)

ru_sentiment <- ru_sentiment %>% 
  mutate(mult = (count * score)) # So this just fundamentally differs from the RTEnglish file in its methodology. It's important to multiply the terms by the frequency, so then we get a true average for the RT Russian search term.

# view(ru_sentiment)

multsum <- tibble(summary(ru_sentiment$mult))

multsum2 <- as.data.frame(t(multsum))
```

```{r}
multsum2 %>%
  gt() %>%
  tab_header(
    title = "Sentiment Polarization Summary",
    subtitle = "RT Search Term: N/A, 'Клинтон' (Clinton) Terms"
    )  %>% 
  tab_source_note(
    source_note = "Note special methodology for Russian sentiment analysis."
     ) %>% 
  cols_label(
    V1 = "Min.",
    V2 = "1st Qu.",
    V3 = "Median",
    V4 = "Mean",
    V5 = "3rd Qu.",
    V6 = "Max."
    ) %>% 
  gtsave("all-klinton/table1.png", zoom = 2.5, expand = 10)
```

```{r}
# No emotion analysis available in Russian for R as far as I can tell.
```
